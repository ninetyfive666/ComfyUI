## ComfyUI-Manager: installing dependencies done.
[2024-09-10 14:50] ** ComfyUI startup time: 2024-09-10 14:50:33.595042
[2024-09-10 14:50] ** Platform: Windows
[2024-09-10 14:50] ** Python version: 3.12.4 (tags/v3.12.4:8e8a4ba, Jun  6 2024, 19:30:16) [MSC v.1940 64 bit (AMD64)]
[2024-09-10 14:50] ** Python executable: C:\Users\Lightmapper\AppData\Local\Programs\Python\Python312\python.exe
[2024-09-10 14:50] ** ComfyUI Path: C:\Users\Lightmapper\Documents\GitHub\ComfyUI
[2024-09-10 14:50] ** Log path: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\comfyui.log
[2024-09-10 14:50] 
Prestartup times for custom nodes:
[2024-09-10 14:50]    2.2 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI-Manager
[2024-09-10 14:50] 
Total VRAM 24576 MB, total RAM 130987 MB
[2024-09-10 14:50] pytorch version: 2.5.0.dev20240726+cu124
[2024-09-10 14:50] Set vram state to: NORMAL_VRAM
[2024-09-10 14:50] Device: cuda:0 NVIDIA GeForce RTX 3090 : cudaMallocAsync
[2024-09-10 14:50] Using pytorch cross attention
[2024-09-10 14:50] [Prompt Server] web root: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\web
[2024-09-10 14:50] C:\Users\Lightmapper\AppData\Local\Programs\Python\Python312\Lib\site-packages\kornia\feature\lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
[2024-09-10 14:50] ### Loading: ComfyUI-Manager (V2.48.5)
[2024-09-10 14:50] ### ComfyUI Revision: 2494 [1b676bf3] | Released on '2024-08-06'
[2024-09-10 14:50] 
Import times for custom nodes:
[2024-09-10 14:50]    0.0 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\websocket_image_save.py
[2024-09-10 14:50]    0.0 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI-SD3-nodes
[2024-09-10 14:50]    0.3 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI-Manager
[2024-09-10 14:50] 
[2024-09-10 14:50] Starting server

[2024-09-10 14:50] To see the GUI go to: http://127.0.0.1:8188
[2024-09-10 14:50] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json
[2024-09-10 14:50] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json
[2024-09-10 14:50] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/github-stats.json
[2024-09-10 14:50] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json
[2024-09-10 14:50] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json
[2024-09-10 14:50] FETCH DATA from: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI-Manager\extension-node-map.json [DONE]
[2024-09-10 14:51] got prompt
[2024-09-10 14:51] model weight dtype torch.float8_e4m3fn, manual cast: torch.bfloat16
[2024-09-10 14:51] model_type FLUX
[2024-09-10 14:51] Using pytorch attention in VAE
[2024-09-10 14:51] Using pytorch attention in VAE
[2024-09-10 14:51] Requested to load FluxClipModel_
[2024-09-10 14:51] Loading 1 new model
[2024-09-10 14:51] C:\Users\Lightmapper\Documents\GitHub\ComfyUI\comfy\ldm\modules\attention.py:407: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:566.)
  out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0, is_causal=False)
[2024-09-10 14:51] Requested to load Flux
[2024-09-10 14:51] Loading 1 new model
[2024-09-10 14:52] 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:27<00:00,  1.36s/it]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:27<00:00,  1.37s/it]
[2024-09-10 14:52] Requested to load AutoencodingEngine
[2024-09-10 14:52] Loading 1 new model
[2024-09-10 14:52] Prompt executed in 58.19 seconds
[2024-09-10 14:52] got prompt
[2024-09-10 14:53] 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:26<00:00,  1.32s/it]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:26<00:00,  1.32s/it]
[2024-09-10 14:53] Prompt executed in 27.73 seconds
[2024-09-10 14:53] got prompt
[2024-09-10 14:53] 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:26<00:00,  1.32s/it]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:26<00:00,  1.30s/it]
[2024-09-10 14:53] Prompt executed in 27.01 seconds
