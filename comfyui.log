## ComfyUI-Manager: installing dependencies done.
[2024-06-27 15:25] ** ComfyUI startup time: 2024-06-27 15:25:09.816581
[2024-06-27 15:25] ** Platform: Windows
[2024-06-27 15:25] ** Python version: 3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]
[2024-06-27 15:25] ** Python executable: C:\Python310\python.exe
[2024-06-27 15:25] ** Log path: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\comfyui.log
[2024-06-27 15:25] 
Prestartup times for custom nodes:
[2024-06-27 15:25]    0.0 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\rgthree-comfy
[2024-06-27 15:25]    1.8 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI-Manager
[2024-06-27 15:25] 
Total VRAM 24576 MB, total RAM 130987 MB
[2024-06-27 15:25] pytorch version: 2.1.2+cu121
[2024-06-27 15:25] Set vram state to: NORMAL_VRAM
[2024-06-27 15:25] Device: cuda:0 NVIDIA GeForce RTX 3090 : cudaMallocAsync
[2024-06-27 15:25] Using pytorch cross attention
[2024-06-27 15:25] Traceback (most recent call last):
  File "C:\Users\Lightmapper\Documents\GitHub\ComfyUI\nodes.py", line 1906, in load_custom_node
    module_spec.loader.exec_module(module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "C:\Users\Lightmapper\Documents\GitHub\ComfyUI\comfy_extras\nodes_upscale_model.py", line 3, in <module>
    from spandrel import ModelLoader, ImageModelDescriptor
ModuleNotFoundError: No module named 'spandrel'

[2024-06-27 15:25] Cannot import C:\Users\Lightmapper\Documents\GitHub\ComfyUI\comfy_extras\nodes_upscale_model.py module for custom nodes: No module named 'spandrel'
[2024-06-27 15:25] ### Loading: ComfyUI-Manager (V2.38.1)
[2024-06-27 15:25] ### ComfyUI Revision: 2280 [0e4d9424] | Released on '2024-06-18'
[2024-06-27 15:25] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json
[2024-06-27 15:25] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json
[2024-06-27 15:25] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/github-stats.json
[2024-06-27 15:25] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json
[2024-06-27 15:25] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json
[2024-06-27 15:25] no module 'xformers'. Processing without...
[2024-06-27 15:25] no module 'xformers'. Processing without...
[2024-06-27 15:25] [36;20m[comfyui_controlnet_aux] | INFO -> Using ckpts path: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\comfyui_controlnet_aux\ckpts[0m
[2024-06-27 15:25] [36;20m[comfyui_controlnet_aux] | INFO -> Using symlinks: False[0m
[2024-06-27 15:25] [36;20m[comfyui_controlnet_aux] | INFO -> Using ort providers: ['CUDAExecutionProvider', 'DirectMLExecutionProvider', 'OpenVINOExecutionProvider', 'ROCMExecutionProvider', 'CPUExecutionProvider', 'CoreMLExecutionProvider'][0m
[2024-06-27 15:25] Traceback (most recent call last):
  File "C:\Users\Lightmapper\Documents\GitHub\ComfyUI\nodes.py", line 1906, in load_custom_node
    module_spec.loader.exec_module(module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI_UltimateSDUpscale\__init__.py", line 32, in <module>
    from .nodes import NODE_CLASS_MAPPINGS, NODE_DISPLAY_NAME_MAPPINGS
  File "C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI_UltimateSDUpscale\nodes.py", line 10, in <module>
    from modules.upscaler import UpscalerData
  File "C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI_UltimateSDUpscale\modules\upscaler.py", line 3, in <module>
    from comfy_extras.nodes_upscale_model import ImageUpscaleWithModel
  File "C:\Users\Lightmapper\Documents\GitHub\ComfyUI\comfy_extras\nodes_upscale_model.py", line 3, in <module>
    from spandrel import ModelLoader, ImageModelDescriptor
ModuleNotFoundError: No module named 'spandrel'

[2024-06-27 15:25] Cannot import C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI_UltimateSDUpscale module for custom nodes: No module named 'spandrel'
[2024-06-27 15:25] Traceback (most recent call last):
  File "C:\Users\Lightmapper\Documents\GitHub\ComfyUI\nodes.py", line 1906, in load_custom_node
    module_spec.loader.exec_module(module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\efficiency-nodes-comfyui\__init__.py", line 9, in <module>
    from  .efficiency_nodes import NODE_CLASS_MAPPINGS
  File "C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\efficiency-nodes-comfyui\efficiency_nodes.py", line 36, in <module>
    from comfy_extras.nodes_upscale_model import UpscaleModelLoader, ImageUpscaleWithModel
  File "C:\Users\Lightmapper\Documents\GitHub\ComfyUI\comfy_extras\nodes_upscale_model.py", line 3, in <module>
    from spandrel import ModelLoader, ImageModelDescriptor
ModuleNotFoundError: No module named 'spandrel'

[2024-06-27 15:25] Cannot import C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\efficiency-nodes-comfyui module for custom nodes: No module named 'spandrel'
[2024-06-27 15:25] 
[2024-06-27 15:25] [92m[rgthree] Loaded 40 magnificent nodes.[0m
[2024-06-27 15:25] [92m[rgthree] Will use rgthree's optimized recursive execution.[0m
[2024-06-27 15:25] 
[2024-06-27 15:25] 
Import times for custom nodes:
[2024-06-27 15:25]    0.0 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\websocket_image_save.py
[2024-06-27 15:25]    0.0 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI-seamless-tiling
[2024-06-27 15:25]    0.0 seconds (IMPORT FAILED): C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\efficiency-nodes-comfyui
[2024-06-27 15:25]    0.0 seconds (IMPORT FAILED): C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI_UltimateSDUpscale
[2024-06-27 15:25]    0.0 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI_IPAdapter_plus
[2024-06-27 15:25]    0.0 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\rgthree-comfy
[2024-06-27 15:25]    0.1 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI-Flowty-LDSR
[2024-06-27 15:25]    0.3 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI-Manager
[2024-06-27 15:25]    0.8 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\comfyui_controlnet_aux
[2024-06-27 15:25]    1.0 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI-Flowty-TripoSR
[2024-06-27 15:25]    1.3 seconds: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI-SUPIR
[2024-06-27 15:25] 
[2024-06-27 15:25] WARNING: some comfy_extras/ nodes did not import correctly. This may be because they are missing some dependencies.

[2024-06-27 15:25] IMPORT FAILED: nodes_upscale_model.py
[2024-06-27 15:25] 
This issue might be caused by new missing dependencies added the last time you updated ComfyUI.
[2024-06-27 15:25] Please do a: pip install -r requirements.txt
[2024-06-27 15:25] 
[2024-06-27 15:25] Starting server

[2024-06-27 15:25] To see the GUI go to: http://127.0.0.1:8188
[2024-06-27 15:25] FETCH DATA from: C:\Users\Lightmapper\Documents\GitHub\ComfyUI\custom_nodes\ComfyUI-Manager\extension-node-map.json [DONE]
[2024-06-27 15:26] got prompt
[2024-06-27 15:26] [32m[rgthree] Using rgthree's optimized recursive execution.[0m
[2024-06-27 15:26] [32m[rgthree][0m First run patching recursive_output_delete_if_changed and recursive_will_execute.[0m
[2024-06-27 15:26] [33m[rgthree] Note: [0mIf execution seems broken due to forward ComfyUI changes, you can disable the optimization from rgthree settings in ComfyUI.[0m
[2024-06-27 15:26] model_type EPS
[2024-06-27 15:26] Using pytorch attention in VAE
[2024-06-27 15:26] Using pytorch attention in VAE
[2024-06-27 15:26] Requested to load SDXLClipModel
[2024-06-27 15:26] Loading 1 new model
[2024-06-27 15:26] Requested to load SDXL
[2024-06-27 15:26] Loading 1 new model
[2024-06-27 15:27] 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:27<00:00,  2.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:27<00:00,  2.18it/s]
[2024-06-27 15:27] Requested to load AutoencoderKL
[2024-06-27 15:27] Loading 1 new model
[2024-06-27 15:27] model_type EPS
[2024-06-27 15:27] Using pytorch attention in VAE
[2024-06-27 15:27] Using pytorch attention in VAE
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_proj_in.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_proj_in.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_proj_in.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_proj_out.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_proj_out.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_proj_out.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_proj_in.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_proj_in.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_proj_in.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_proj_out.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_proj_out.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_proj_out.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_proj_in.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_proj_in.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_proj_in.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_proj_out.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_proj_out.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_proj_out.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_proj_in.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_proj_in.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_proj_in.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_proj_out.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_proj_out.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_proj_out.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_proj_in.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_proj_in.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_proj_in.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_proj_out.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_proj_out.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_proj_out.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_proj_in.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_proj_in.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_proj_in.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_proj_out.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_proj_out.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_proj_out.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_proj_in.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_proj_in.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_proj_in.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_proj_out.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_proj_out.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_proj_out.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_proj_in.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_proj_in.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_proj_in.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_proj_out.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_proj_out.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_proj_out.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.alpha
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight
[2024-06-27 15:27] lora key not loaded: lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight
[2024-06-27 15:27] Requested to load SDXLClipModel
[2024-06-27 15:27] Loading 1 new model
[2024-06-27 15:27] Requested to load SDXL
[2024-06-27 15:27] Loading 1 new model
[2024-06-27 15:27] ERROR lora diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight shape '[640, 2048]' is invalid for input of size 491520
[2024-06-27 15:27] ERROR lora diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight shape '[640, 2048]' is invalid for input of size 491520
[2024-06-27 15:27] ERROR lora diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight shape '[640, 2048]' is invalid for input of size 491520
[2024-06-27 15:27] ERROR lora diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight shape '[640, 2048]' is invalid for input of size 491520
[2024-06-27 15:27] ERROR lora diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight shape '[1280, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight shape '[1280, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight shape '[1280, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight shape '[1280, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight shape '[1280, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight shape '[1280, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.proj_in.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.proj_out.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight shape '[5120, 640]' is invalid for input of size 13107200
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight shape '[640, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight shape '[640, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight shape '[640, 2560]' is invalid for input of size 6553600
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.proj_in.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.proj_out.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight shape '[5120, 640]' is invalid for input of size 13107200
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight shape '[640, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight shape '[640, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight shape '[640, 2560]' is invalid for input of size 6553600
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.proj_in.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.proj_out.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight shape '[5120, 640]' is invalid for input of size 13107200
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight shape '[640, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight shape '[640, 2048]' is invalid for input of size 983040
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight shape '[640, 640]' is invalid for input of size 1638400
[2024-06-27 15:27] ERROR lora diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight shape '[640, 2560]' is invalid for input of size 6553600
[2024-06-27 15:27] 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:20<00:00,  1.37it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:20<00:00,  1.43it/s]
[2024-06-27 15:27] Requested to load AutoencoderKL
[2024-06-27 15:27] Loading 1 new model
[2024-06-27 15:27] Prompt executed in 96.12 seconds
[2024-06-27 15:29] got prompt
[2024-06-27 15:29] [32m[rgthree] Using rgthree's optimized recursive execution.[0m
[2024-06-27 15:29] Requested to load SDXLClipModel
[2024-06-27 15:29] Loading 1 new model
[2024-06-27 15:29] Requested to load SDXL
[2024-06-27 15:29] Loading 1 new model
[2024-06-27 15:29] 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:23<00:00,  2.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:23<00:00,  2.52it/s]
[2024-06-27 15:30] 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:18<00:00,  1.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:18<00:00,  1.59it/s]
[2024-06-27 15:30] Prompt executed in 47.76 seconds
[2024-06-27 15:31] got prompt
[2024-06-27 15:31] [32m[rgthree] Using rgthree's optimized recursive execution.[0m
[2024-06-27 15:31] 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:24<00:00,  2.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:24<00:00,  2.49it/s]
[2024-06-27 15:32] 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:18<00:00,  1.63it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:18<00:00,  1.59it/s]
[2024-06-27 15:32] Prompt executed in 45.14 seconds
[2024-06-27 15:33] got prompt
[2024-06-27 15:33] [32m[rgthree] Using rgthree's optimized recursive execution.[0m
[2024-06-27 15:33] Diffusion using fp16
[2024-06-27 15:33] Diffusion using bf16
[2024-06-27 15:33] Encoder using bf16
[2024-06-27 15:33] Using non-tiled sampling
[2024-06-27 15:33] making attention of type 'vanilla' with 512 in_channels
[2024-06-27 15:33] Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[2024-06-27 15:33] making attention of type 'vanilla' with 512 in_channels
[2024-06-27 15:33] Attempting to load SUPIR model: [C:\Users\Lightmapper\Documents\GitHub\ComfyUI\models\checkpoints\SUPIR-v0F_fp16.safetensors]
[2024-06-27 15:33] Loaded state_dict from [C:\Users\Lightmapper\Documents\GitHub\ComfyUI\models\checkpoints\SUPIR-v0F_fp16.safetensors]
[2024-06-27 15:33] Attempting to load SDXL model: [C:\Users\Lightmapper\Documents\GitHub\ComfyUI\models\checkpoints\Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors]
[2024-06-27 15:33] Loaded state_dict from [C:\Users\Lightmapper\Documents\GitHub\ComfyUI\models\checkpoints\Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors]
[2024-06-27 15:34] Loading first clip model from SDXL checkpoint
[2024-06-27 15:34] Loading second clip model from SDXL checkpoint
[2024-06-27 15:34] captions:  ['']
[2024-06-27 15:34] Sampler:  .sgm.modules.diffusionmodules.sampling.RestoreDPMPP2MSampler
[2024-06-27 15:34] sampler_config:  {'num_steps': 40, 'restore_cfg': -1.0, 's_churn': 5, 's_noise': 1.003, 'discretization_config': {'target': '.sgm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization'}, 'guider_config': {'target': '.sgm.modules.diffusionmodules.guiders.LinearCFG', 'params': {'scale': 4.0, 'scale_min': 4.0}}}
[2024-06-27 15:34] Seed set to 3494253987
[2024-06-27 15:34] [Tiled VAE]: input_size: torch.Size([1, 3, 4096, 8192]), tile_size: 512, padding: 32
[2024-06-27 15:34] [Tiled VAE]: split to 8x16 = 128 tiles. Optimal tile size 512x512, original tile size 512x512
[2024-06-27 15:34] [Tiled VAE]: Executing Encoder Task Queue: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11648/11648 [00:43<00:00, 270.54it/s]
[2024-06-27 15:34] [Tiled VAE]: Done in 43.496s, max VRAM alloc 1022.141 MB
[2024-06-27 15:34] [Tiled VAE]: input_size: torch.Size([1, 4, 512, 1024]), tile_size: 64, padding: 11
[2024-06-27 15:34] [Tiled VAE]: split to 8x16 = 128 tiles. Optimal tile size 64x64, original tile size 64x64
[2024-06-27 15:36] [Tiled VAE]: Executing Decoder Task Queue: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 15667/15744 [01:44<00:00, 142.17it/s][Tiled VAE]: Executing Decoder Task Queue: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 15682/15744 [01:44<00:00, 130.02it/s][Tiled VAE]: Executing Decoder Task Queue: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 15697/15744 [01:44<00:00, 133.99it/s][Tiled VAE]: Executing Decoder Task Queue: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 15715/15744 [01:44<00:00, 135.31it/s][Tiled VAE]: Executing Decoder Task Queue: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 15730/15744 [01:45<00:00, 137.40it/s][Tiled VAE]: Executing Decoder Task Queue: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15744/15744 [01:45<00:00, 149.74it/s]
[2024-06-27 15:36] [Tiled VAE]: Done in 105.532s, max VRAM alloc 1612.282 MB
[2024-06-27 15:36] [Tiled VAE]: input_size: torch.Size([1, 3, 4096, 8192]), tile_size: 512, padding: 32
[2024-06-27 15:36] [Tiled VAE]: split to 8x16 = 128 tiles. Optimal tile size 512x512, original tile size 512x512
[2024-06-27 15:37] [Tiled VAE]: Executing Encoder Task Queue: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11648/11648 [00:44<00:00, 261.37it/s]
[2024-06-27 15:37] [Tiled VAE]: Done in 45.126s, max VRAM alloc 1410.141 MB
[2024-06-27 15:37] Using local prompt: 
[2024-06-27 15:37] ['high quality, detailed, hdri, skymap, italia, panorama, riomaggiore']
